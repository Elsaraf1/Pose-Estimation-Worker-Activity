{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4f6c12-143e-41d3-b406-9f8ef89a9e21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b54fa44b-4bcf-4684-bef6-69bf041a7c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import math\n",
    "import mediapipe as mp \n",
    "from yolov5 import YOLOv5\n",
    "import cv2\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import warnings \n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0294932f-b852-4fac-9e52-04c4a06893cb",
   "metadata": {},
   "source": [
    "## Constant "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "025bff2a-6da4-465f-8d2a-6b6587be026f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_video = './20211118173000-20211118183000/car cam.mp4'\n",
    "output_video = './20211118173000-20211118183000/car cam processed.mp4'\n",
    "\n",
    "width,height = 1903,945\n",
    "fbs = 10\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_video,fourcc, fbs,(width, height))\n",
    "\n",
    "worker_activity = {}  # Format: {person_id: {'standing': 0, 'sitting': 0, 'moving': 0}} # to track the activites for person \n",
    "\n",
    "landmark_data = [] # create landmarks to save the landmarks for csv file \n",
    "output_dir = './cropped_landmarks'\n",
    "\n",
    "# for checking if this the first frame for the person\n",
    "person_stabilization = {}  # Format: {person_id: stabilization_frames}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6dc9b256-77cd-47f1-bfa8-8da145a8cb98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5  2024-11-25 Python-3.11.5 torch-2.5.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 270 layers, 7235389 parameters, 0 gradients, 16.6 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load YOLO model\n",
    "yolo_model = YOLOv5(\"yolov5s.pt\")  # Pre-trained YOLOv5 model\n",
    "\n",
    "# Initialize MediaPipe Pose\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose()\n",
    "\n",
    "# Dictionary to store landmarks from the previous frame\n",
    "prev_landmarks = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80c2bc9f-896d-4a32-86ce-90561c7b00f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute Euclidean distance\n",
    "def euclidean_distance(point1, point2):\n",
    "    return math.sqrt((point1[0] - point2[0])**2 + (point1[1] - point2[1])**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a6e7600-e796-4e5a-8775-c23f69bff4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_activity (landmarks, prev_landmarks, bbox_id):\n",
    "    text = ''\n",
    "    left_hip = landmarks[mp_pose.PoseLandmark.LEFT_HIP]\n",
    "    right_hip = landmarks[mp_pose.PoseLandmark.RIGHT_HIP]\n",
    "    left_knee = landmarks[mp_pose.PoseLandmark.LEFT_KNEE]\n",
    "    right_knee = landmarks[mp_pose.PoseLandmark.RIGHT_KNEE]\n",
    "    left_shoulder = landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER]\n",
    "    right_shoulder = landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER]\n",
    "    \n",
    "    # Compute average positions for hips, knees, and shoulders\n",
    "    avg_hip_y = (left_hip.y + right_hip.y) / 2\n",
    "    avg_knee_y = (left_knee.y + right_knee.y) / 2\n",
    "    avg_shoulder_y = (left_shoulder.y + right_shoulder.y) / 2\n",
    "    current_shoulder = ((left_shoulder.x + right_shoulder.x) / 2, avg_shoulder_y)\n",
    "    movement_detection = False\n",
    "    if bbox_id in prev_landmarks:\n",
    "        prev_shoulder = prev_landmarks[bbox_id]\n",
    "        displacement = euclidean_distance(current_shoulder, prev_shoulder)\n",
    "        if displacement >0.2:\n",
    "            movement_detection = True\n",
    "    prev_landmarks[bbox_id] = current_shoulder\n",
    "        \n",
    "    if movement_detection:\n",
    "        text=  'moving'\n",
    "    elif abs(avg_hip_y - avg_knee_y) < 0.1 and avg_hip_y > avg_shoulder_y:\n",
    "        text= \"sitting\"\n",
    "    elif avg_hip_y < avg_knee_y and avg_shoulder_y < avg_hip_y:\n",
    "        text= 'standing'\n",
    "    else:\n",
    "        text= 'unknown'\n",
    "\n",
    "    return text, prev_landmarks\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cce2830-ca27-4f0f-a0db-53f2b6b5b677",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_landmark(landmarks,activity):\n",
    "    left_hip = landmarks[mp_pose.PoseLandmark.LEFT_HIP]\n",
    "    right_hip = landmarks[mp_pose.PoseLandmark.RIGHT_HIP]\n",
    "    left_knee = landmarks[mp_pose.PoseLandmark.LEFT_KNEE]\n",
    "    right_knee = landmarks[mp_pose.PoseLandmark.RIGHT_KNEE]\n",
    "    left_shoulder = landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER]\n",
    "    right_shoulder = landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER]\n",
    "    return {'idx':idx,\n",
    "                           \"left_hip_x\": left_hip.x,\n",
    "                          'right_hip_x':right_hip.x,\n",
    "                          'left_knee_x':left_knee.x,\n",
    "                          'right_knee_x':right_knee.x,\n",
    "                          'left_shoulder_x':left_shoulder.x,\n",
    "                          'right_shoulder_x':right_shoulder.x,\n",
    "                          \"left_hip_y\": left_hip.y,\n",
    "                          'right_hip_y':right_hip.y,\n",
    "                          'left_knee_y':left_knee.y,\n",
    "                          'right_knee_y':right_knee.y,\n",
    "                          'left_shoulder_y':left_shoulder.y,\n",
    "                          'right_shoulder_y':right_shoulder.y,\n",
    "                          \"left_hip_z\": left_hip.z,\n",
    "                          'right_hip_z':right_hip.z,\n",
    "                          'left_knee_z':left_knee.z,\n",
    "                          'right_knee_z':right_knee.z,\n",
    "                          'left_shoulder_z':left_shoulder.z,\n",
    "                          'right_shoulder_z':right_shoulder.z,\n",
    "                          \"left_hip_v\": left_hip.visibility,\n",
    "                          'right_hip_v':right_hip.visibility,\n",
    "                          'left_knee_v':left_knee.visibility,\n",
    "                          'right_knee_v':right_knee.visibility,\n",
    "                          'left_shoulder_v':left_shoulder.visibility,\n",
    "                          'right_shoulder_v':right_shoulder.visibility,\n",
    "                          \n",
    "                          'avg_hip_y' : (left_hip.y + right_hip.y) / 2,\n",
    "                          \"avg_knee_y\": (left_knee.y + right_knee.y) / 2,\n",
    "                          'avg_shoulder_y':(left_shoulder.y + right_shoulder.y) / 2,\n",
    "    \n",
    "                           \"label\": activity}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14ed4c24-46f5-4a12-a0a0-fe695d2545a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_img(img, idx, dir_path):\n",
    "    output_path = os.path.join(dir_path, f\"person_{idx}.jpg\")\n",
    "    cv2.imwrite(output_path, cv2.cvtColor(img, cv2.COLOR_RGB2BGR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "911cd9e1-96eb-4f62-94d2-c1ca3d0ff60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# starting the video \n",
    "cap = cv2.VideoCapture(input_video)\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    frame = cv2.resize(frame, (width,height))\n",
    "    if not ret:\n",
    "        break\n",
    "    # step one detect the human by YOLO & create the rectangle \n",
    "    detections = yolo_model.predict(frame)\n",
    "    detections = detections.xyxy[0]\n",
    "    for idx, (*box, conf, cls) in enumerate(detections.tolist()):\n",
    "        x1, y1, x2, y2 = map(int, box)   # Bounding box coordinates\n",
    "        if int(cls) == 0: # check if the object is human\n",
    "            # cv2.rectangle(frame, (x1,y1),(x2,y2), (0,255,0),2)\n",
    "            person_id = idx\n",
    "            if person_id not in person_stabilization: # add the person if this is first time \n",
    "                person_stabilization[person_id] = 0\n",
    "        \n",
    "            # Step 2: Crop region for each person\n",
    "            person_crop = frame[y1:y2,x1:x2]\n",
    "            if person_crop.size>0:\n",
    "                rgb_crop = cv2.cvtColor(person_crop, cv2.COLOR_BGR2RGB)\n",
    "                results = pose.process(rgb_crop)\n",
    "                \n",
    "                # Draw pose landmarks on the cropped person, and save the landmarks, create activities and label it \n",
    "                if results.pose_landmarks:  \n",
    "                    if person_stabilization[person_id]<5:\n",
    "                        person_stabilization[person_id]+=1\n",
    "                        continue\n",
    "                        \n",
    "                    landmarks = results.pose_landmarks.landmark\n",
    "                    # Draw the landmarks\n",
    "                    mp.solutions.drawing_utils.draw_landmarks(\n",
    "                        frame[y1:y2, x1:x2], results.pose_landmarks, mp_pose.POSE_CONNECTIONS\n",
    "                    )\n",
    "\n",
    "                    \n",
    "                    # make the activity \n",
    "                    activity, prev_landmarks = classify_activity(landmarks, prev_landmarks, idx)\n",
    "                    label = f\"{activity.upper()}\"\n",
    "                    cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n",
    "\n",
    "                    # Count the activity\n",
    "                    if idx not in worker_activity:\n",
    "                        worker_activity[idx]= {'standing': 0, 'sitting': 0, 'moving': 0, 'unknown':0}\n",
    "                    worker_activity[idx][activity] +=1\n",
    "                    \n",
    "                    # save the img\n",
    "                    save_img(rgb_crop, idx, output_dir)\n",
    "                    \n",
    "                    #extract the landmarks\n",
    "                    landmark_data.append(extract_landmark(landmarks, activity))\n",
    "\n",
    "\n",
    "    # Display the frame with bounding boxes and poses\n",
    "    out.write(frame)\n",
    "    cv2.imshow(\"Multi-Worker Activity Recognition\", frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6408a7-f790-48fd-8245-12f63cab06e7",
   "metadata": {},
   "source": [
    "## Calculate Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da779d95-9d6b-4e76-bb9d-bfbf0ad98d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "frame_to_minutes = 1/(fbs * 60)\n",
    "workers_time = pd.DataFrame(worker_activity).T\n",
    "workers_time *= frame_to_minutes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27598a32-7c0c-4bbd-a86d-aff26326aade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "standing    0.069444\n",
       "sitting     0.031111\n",
       "moving      0.008889\n",
       "unknown     0.000556\n",
       "dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workers_time.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "850914ae-dfa7-4ed9-9650-61b70a559dae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(cap.get(cv2.CAP_PROP_FPS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eed7f37f-c085-45ec-89e1-6e3314833d89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2af9a782-4e79-428c-81e4-989fccf7bac2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6058aa01-9cd9-4d09-b19b-cad11f35a954",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
